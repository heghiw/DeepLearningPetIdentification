{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1jtGwb4E4dPDfliSxs517",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heghiw/DeepLearningPetIdentification/blob/main/fine54.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYrA3msXSuK9",
        "outputId": "50b875da-c103-47b2-b48a-aeff915f1616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utility.py downloaded successfully.\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ],
      "source": [
        "## 1st -  Download utility.py file from github repository\n",
        "## 2nd - Imports all functions from utility.py\n",
        "\n",
        "import requests\n",
        "\n",
        "# Correct raw URL for the utility.py file\n",
        "url = \"https://raw.githubusercontent.com/avkaz/DeepLearningPetIdentification/main/utility.py\"\n",
        "\n",
        "# Fetch and save the file locally\n",
        "response = requests.get(url)\n",
        "with open(\"utility.py\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "\n",
        "import utility\n",
        "print(\"utility.py downloaded successfully.\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "!pip install faiss-cpu\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "import faiss\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')"
      ],
      "metadata": {
        "id": "WUVZ5qFxUCv9"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = utility.get_data()"
      ],
      "metadata": {
        "id": "139d0bBoUGNm"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = {k: v for k, v in data.items() if len(v['images']) > 3}"
      ],
      "metadata": {
        "id": "D5jRPygbmBJl"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter out pets for training, only using postings that have 3+ pictures for triplet loss"
      ],
      "metadata": {
        "id": "-eJvKXonoY8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nelen=int( 0.8*len(filtered_data))"
      ],
      "metadata": {
        "id": "e5y0XGtMa2lD"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(filtered_data.keys())\n",
        "train_keys, test_keys = train_test_split(keys, train_size=nelen, random_state=42)\n",
        "train_data = {k: filtered_data[k] for k in train_keys}\n",
        "test_data = {k: filtered_data[k] for k in test_keys}"
      ],
      "metadata": {
        "id": "pKdLMoe0ootL"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "id": "XD-kftlkNe1u",
        "outputId": "45614d40-d912-4196-8997-003cbd194ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "363"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download preprocess"
      ],
      "metadata": {
        "id": "67kSABnNo9pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(data):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for pet_id, pet_info in data.items():\n",
        "        for img_url in pet_info['images']:\n",
        "            try:\n",
        "                img = utility.download_and_preprocess_image(img_url)\n",
        "                images.append(img)\n",
        "                labels.append(pet_id)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image {img_url}: {e}\")\n",
        "    return tf.convert_to_tensor(images), labels\n",
        "\n",
        "train_images, train_labels = preprocess_images(train_data)\n",
        "test_images, test_labels = preprocess_images(test_data)"
      ],
      "metadata": {
        "id": "rstTqVHgo9Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.losses import Loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "import faiss\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Assuming utility.py with download_and_preprocess_image is imported\n",
        "# from utility import download_and_preprocess_image\n",
        "\n",
        "\n",
        "def filter_pets_by_images(data, min_images=3):\n",
        "    \"\"\"Filters pets with at least 'min_images' images.\"\"\"\n",
        "    return {key: value for key, value in data.items() if len(value['images']) >= min_images}\n",
        "\n",
        "\n",
        "def triplet_loss(margin=1.0):\n",
        "    \"\"\"Triplet Loss function.\"\"\"\n",
        "    def loss(y_true, y_pred):\n",
        "        anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "        positive_distance = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "        negative_distance = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "        loss_value = tf.maximum(positive_distance - negative_distance + margin, 0.0)\n",
        "        return tf.reduce_mean(loss_value)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create the EfficientNetB0 model for feature extraction.\"\"\"\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
        "    inputs = tf.keras.Input(shape=(128, 128, 3))\n",
        "    x = base_model(inputs)\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_embeddings(model, image_urls):\n",
        "    \"\"\"Generate embeddings for the list of image URLs.\"\"\"\n",
        "    embeddings = []\n",
        "    for url in image_urls:\n",
        "        img = utility.download_and_preprocess_image(url, target_size=(128, 128))\n",
        "        emb = model.predict(np.expand_dims(img, axis=0))\n",
        "        embeddings.append(emb.flatten())\n",
        "    return np.array(embeddings)"
      ],
      "metadata": {
        "id": "g6umXBUKpx8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter pets with at least 3 images\n",
        "filtered_data = filter_pets_by_images(data, min_images=3)\n",
        "model = create_model()\n",
        "# Prepare the data for training and testing\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for pet_key, pet_info in filtered_data.items():\n",
        "    # Use the first 3 images for each pet as an example\n",
        "    images = pet_info['images']\n",
        "    embeddings = generate_embeddings(model, images)\n",
        "    label = pet_key  # Using the pet's key as the label (you can modify this)\n",
        "\n",
        "    # Append embeddings and corresponding label\n",
        "    all_images.extend(embeddings)\n",
        "    all_labels.extend([label] * len(embeddings))\n",
        "\n"
      ],
      "metadata": {
        "id": "3GDvplF0pqr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(all_images), np.array(all_labels), test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vs8h7rZ2sEXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize FAISS index for similarity search\n",
        "index = faiss.IndexFlatL2(X_train.shape[1])  # L2 distance\n",
        "index.add(X_train)  # Add training data embeddings to the FAISS index\n",
        "\n",
        "# Build the model\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss=triplet_loss(margin=1.0))\n",
        "\n",
        "# Train the model with Triplet Loss\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# For evaluation or similarity search\n",
        "query = generate_embeddings(model, [X_test[0]])  # Example query: the first image from the test set\n",
        "D, I = index.search(query, k=5)  # Search for the 5 nearest neighbors\n",
        "print(\"Nearest neighbors (indices):\", I)\n",
        "print(\"Distances:\", D)"
      ],
      "metadata": {
        "id": "BKQZvyx_tfSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}